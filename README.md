# 03240344_MLA202_practical_test
This repository contains the solution for the three problems:

**Q-Learning on GridWorld:**
The task involves applying Q-learning to a 5x5 GridWorld where the agent starts at the top-left corner and aims to reach the goal at the bottom-right while avoiding walls. The agent learns an optimal policy through interaction with the environment by updating its Q-table based on rewards from actions. After training, the learned policy is visualized, showing the agent's preferred actions in each grid cell. The agent’s performance is then evaluated by testing its success rate and the number of steps it takes to reach the goal. Finally, the agent’s learning progress is plotted, displaying its performance over the course of the episodes. The implementation includes setting up the environment, training the agent, visualizing the policy, and assessing the agent’s performance.

**Deep Q-Network (DQN) for cartpole:**
The task involves implementing a Deep Q-Network (DQN) to solve the CartPole-v1 environment from OpenAI's Gym. The agent must learn to balance a pole on a cart by selecting actions based on its current state. The solution includes creating a neural network for the Q-function, experience replay for efficient learning, and a target network to stabilize training. The agent is trained over 500 episodes, and its performance is evaluated based on rewards and losses. After training, the agent is tested for its ability to balance the pole in multiple episodes. The results, including training rewards and losses, are visualized in plots, and the agent's performance is summarized with key statistics. The goal is to train the agent to reach an average reward of at least 475 over the last 100 episodes, indicating it has solved the task.

**Debugging and comparing Q-learning:**
The task in this problem is to identify and fix bugs in a Q-learning implementation for the FrozenLake environment. The buggy code contains issues with Q-table initialization, action selection (using np.argmin instead of np.argmax), the Q-learning update rule (missing learning rate), and the lack of epsilon decay for exploration. The goal is to debug the code, correct these issues, and compare the performance of the buggy and fixed versions by training both over multiple episodes. The comparison includes tracking success rates, plotting learning progress, and evaluating the improvement after fixing the bugs.
